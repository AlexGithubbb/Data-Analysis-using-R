library("readr")
library("forecast")
library("dplyr")
library("TTR")
amzn <- read.csv("AMZN.csv", TRUE, ",")
install.packages("readr")
head(amzn)
head(amzn,7)
head(amzn)
#e2
amzn_ts <- ts()
#e2
amzn_ts <- ts(amzn$Close, frequency = 12, start = c(2008,3))
amzn_ts
#e2
amzn_ts <- ts(rev(amzn$Close), frequency = 12, start = c(2008,3))
amzn_ts
plot.ts(amzn_ts)
#e3
amzn_closing_log <- log(amzn_ts)
amzn_closing_log
plot(amzn_closing_log)
#e4
amzn_ts_decompose <- decompose(amzn_ts)
plot(amzn_ts_decompose)
#e5
amzn_smooth <- HoltWinters(amzn_ts, gamma = F)
amzn_smooth
amzn_forecast <- forecast(amzn_smooth)
plot(amzn_forecast)
plot.ts(amzn_closing_log)
plot(amzn_closing_log)
kings1 <- ts(kings, frequency = 4, start = c(1845,1))
plot.ts(kings1)
kingsSMA3 <- SMA(kings, n=8)
plot.ts(kingsSMA3)
kings1 <- ts(kings, frequency = 4, start = c(1845,1))
plot.ts(kings1)
kingsSMA3 <- SMA(kings, n=8)
plot.ts(kingsSMA3)
#e2
amzn_ts <- ts(rev(amzn$Close), frequency = 12, start = c(2008,3))
amzn_ts
plot.ts(amzn_ts)
amzn_tsSMA3 = SMA(amzn_ts, n=3)
plot(amzn_tsSMA3)
plot.ts(amzn_ts)
amzn_tsSMA3 = SMA(amzn_ts, n=3)
plot(amzn_tsSMA3)
Needed <- c("tm","SnowballCC","RColorBrewer","ggplot2","wordcloud","biclust","igraph","fpc")
install.packages(Needed, dependencies = T)
library("tm")
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/",type = "source")
setwd("/Users/Alexpower/Desktop/R/task5")
reviews <- read.csv("Reviews.csv",stringsAsFactors = FALSE)
install.packages(Needed, dependencies = T)
reviews <- read.csv("Reviews.csv",stringsAsFactors = FALSE)
head(reviews)
View(reviews)
reviews$text
review_text <- paste(reviews$text, collapse ="")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
writeLines(as.character(corpus))
#=========================================
# Task5 Exercise 2: Change the type of all the attributes into factor:
sms$type <- as.factor(sms$type)
library("tm")
library("NLP")
sms <- read.csv("spam_ham.csv",stringsAsFactors = FALSE)
head(sms)
str(sms)
summary(sms)
sms$text
#=========================================
# Task5 Exercise 2: Change the type of all the attributes into factor:
sms$type <- factor(sms$type)
# Task5 Exercise 3: Inspect how many Spam and Pam at the converted corpus:
table(sms$type)
#=========================================
# Task5 Exercise 2: Change the type of all the attributes into factor:
sms$type <- as.factor(sms$type)
# Task5 Exercise 3: Inspect how many Spam and Pam at the converted corpus:
table(sms$type)
#=========================================
# Task5 Exercise 2: Change the type of all the attributes into factor:
sms$type <- as.factor(sms$type)
# Task5 Exercise 3: Inspect how many Spam and Pam at the converted corpus:
table(sms$type)
# Task5 Exercise 4: Building a corpus by using 'tm' package
#corpus <- Corpus(sms_source)
corpus <- Corpus(VectorSource(sms$text))
print(corpus)
#=========================================
# Task5 Exercise 5: Inspect the first 3 messages at the corpus
inspect(corpus[1:3])
# Task5 Exercise 6: cleaning the text for corpus
getTransformations()
head(sms,3)
class(sms)
attributes <- as.factor(sms$type)
type <- as.factor(sms$type)
type
table(type)
getTransformations()
corpus <- Corpus(VectorSource(sms$text))
inspect(corpus[1:3])
corpus <- Corpus(VectorSource(sms$text)) # buid a corpus
corpus <- tm_map(text, removeNumbers)
corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1:3])
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus)
# create the dtm for the cleaned corpus
dtm <- DocumentTermMatrix(corpus)
str(dtm)
print(dtm)
dtm
# inspect the dtm
inspect(dtm)
#=========================================
# Task5 Exercise 9: split the dtm into trainning (80%) and testing (20%)
dim(dtm)
5573*0.8
# split dtm into training(0.8) and testing (0.2)
dim(dtm)
5573*0.8
5574*0.8
dtm_train <- dtm[1:4459,]
dtm_test <- dtm[4460:5574,]
print(dtm_train)
print(dtm_test)
class(dtm)
# inspect the dtm
inspect(dtm)
print(dtm_train)
print(dtm_test)
corpus_train <- corpus[1:4459,]
corpus_test <- corpus[4460:5574,]
corpus_train <- corpus[1:4459]
corpus_test <- corpus[4460:5574]
print(corpus_train)
print(corpus_test)
wordcloud(corpus_train,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
corpus_train = corpus[1:4459]
corpus_test  = corpus[4459:5574]
wordcloud(corpus_train,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
library("wordcloud")
#=========================================
# Task5 Exercise 10: wordcloud printing
install.packages("wordcloud")
install.packages("wordcloud")
library("wordcloud")
wordcloud(corpus_train,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(3, 0.5, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(7, 0.5, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.8, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 2, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 9))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 1))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 3))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
wordcloud(corpus_train,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
wordcloud(corpus_test,
max.words=100,     # look at the 100 most common words
scale=c(4, 0.5, 7))
# use 'wordcloud' package to visualize the dtm
install.packages("wordcloud")
install.packages("wordcloud")
library(wordcloud)
wordcloud(corpus_train,
max.word = 100,
scale())
wordcloud(corpus_train,
max.word = 100,
scale(4, 0.5, 7))
corpus_train <- corpus[1:4459]
wordcloud(corpus_train,
max.word = 100,
scale(4, 0.5, 7))
wordcloud(corpus_train,
max.words = 100,
scale(4, 0.5, 7))
wordcloud(corpus_train,
max.words = 100,
scale =  c(4, 0.5, 7))
wordcloud(corpus_test,
max.words = 100,
scale = c(4, 0.5, 7))
wordcloud(corpus_train,
max.words = 100,
scale = c(4, 0.5, 7))
wordcloud(corpus_test,
max.words = 100,
scale = c(4, 0.5, 7))
#
freq_dtm_train <- findFreqTerms(dtm_train, lowfreq = 5)
freq_dtm_train
#=========================================
# Task5 Exercise 12: use the library e1071 to train a Naive Bayes(NB) classifier and predict the outcome of the spam classifier using the testing dtm
freq_terms = findFreqTerms(dtm_train, 5)
freq_terms
freq_terms_train
# Task5 Exercise 11: Print the frequent words at training dtm
freq_terms_train <- findFreqTerms(dtm_train, lowfreq = 5)
freq_terms_train
reduced_dtm_train = DocumentTermMatrix(corpus_train, list(dictionary=freq_terms))
reduced_dtm_test =  DocumentTermMatrix(corpus_test, list(dictionary=freq_terms))
ncol(reduced_dtm_train)
findAssocs(dtm_train, "project",0.6)
findAssocs(dtm, "project",0.6)
# create the dtm for the cleaned corpus
dtm <- DocumentTermMatrix(corpus)
str(dtm)
print(dtm)
findAssocs(dtm, "project",0.6)
# Mining the corpus
freq <- colSums(as.matrix(dtm)) # convert TDM into a mathematical matrix using the as.matrix().
length(freq)
# create sort order(descending)
ord <- order(freq,decreasing = T)
#inspect most frequently occuring terms
freq[head(ord)]
#inspect least frequently occuring terms
freq[tail(ord)]
library("SnowballC")
# Stem document
docs <- tm_map(docs, stemDocument)
# Analyzing Blog Text Posts
# CreateCorpus
docs <- Corpus(DirSource("textmining"))
docs
# Data Cleasing
getTransformations()
# create the toSpace content transformer
toSpace <- content_transformer(function(x,pattern){return(gsub(pattern,"",x))})
docs <- tm_map(docs, toSpace,"-")
docs <- tm_map(docs, toSpace,":")
writeLines(as.character(docs[[30]]))
# Remove punctuation - replace punctuation marks with" "
docs <- tm_map(docs, removePunctuation)
#docs <- tm_map(docs,toSpace,"'")
#docs <- tm_map(docs,toSpace,"'")
#docs <- tm_map(docs,toSpace,"-")
# Transform to lower case(need to wrap in content_transformer)
docs <- tm_map(docs, content_transformer(tolower))
# Strip digits(std transformation, so no need for content_transformer)
docs <- tm_map(docs, removeNumbers)
# remove stopwords using std list in tm
docs <- tm_map(docs, removeWords, stopwords("english"))
# Strip whitespaces(cosmetic?)
docs <- tm_map(docs,stripWhitespace)
# ============Stemming==========
writeLines(as.character(docs[[30]]))
# clean the corpus
corpus <- tem_map(corpus, stemDocument)
# clean the corpus
corpus <- tm_map(corpus, stemDocument)
writeLines(as.character(corpus[[30]]))
writeLines(as.character(corpus[[300]]))
corpus <- Corpus(VectorSource(sms$text)) # buid a corpus
inspect(corpus[1:3])
# clean the corpus
corpus <- tm_map(corpus, stemDocument)
writeLines(as.character(corpus[[300]]))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus)
# Stem document
docs <- tm_map(docs, stemDocument)
writeLines(as.character(docs[[30]]))
writeLines(as.character(corpus[[30]]))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus)
# create the dtm for the cleaned corpus
dtm <- DocumentTermMatrix(corpus)
str(dtm)
print(dtm)
# clean the corpus
corpus <- tm_map(corpus, stemDocument)
writeLines(as.character(corpus[[3000]]))
writeLines(as.character(corpus[[3]]))
writeLines(as.character(corpus[[1]]))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus)
corpus <- Corpus(VectorSource(sms$text)) # buid a corpus
inspect(corpus[1:3])
inspect(corpus[1:5])
corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1:5])
# clean the corpus
#corpus <- tm_map(corpus, stemDocument)
#writeLines(as.character(corpus[[1]]))
corpus <- tm_map(corpus, content_transformer(tolower))
inspect(corpus[1:5])
corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:10])
corpus <- tm_map(corpus, stripWhitespace)
inspect(corpus[1:10])
# dendrogram
library(datasets)
library(MASS)
data("mtcars")
test_vars <- c("mpg","cyl","disp","hp","wt","gear","carb")
pairs(x= mtcars[,test_vars],
panel = panel.smooth,
main = "MTCars Attributes")
hc = hclust(dist(mtcars))
plot(hc)
hcd = as.dendrogram(hc)
plot(hcd)
plot(hcd, type = "triangle")
plot(hcd, type = "triangle")
pairs(x= mtcars[,test_varss],
panel = panel.smooth,
main= "asdf")
test_varss <- c("mpg","cyl","disp","hp","wt","gear","carb")
pairs(x= mtcars[,test_varss],
panel = panel.smooth,
main= "asdf")
# prepare hierarchical cluster
hc = hclust(dis(mtcars))
#very simple dengrogram
plot(hc)
# prepare hierarchical cluster
hc = hclust(dis(mtcars))
#very simple dengrogram
plot(hc)
# using dendrogram objects
hcd = as.dendrogram(hc)
plot(hcd, type = "traingle")
# using dendrogram objects
hcd = as.dendrogram(hc)
plot(hcd, type = "traingle")
plot(hcd, type = "triangle")
plot(hcd, type = "fan")
plot(hcd, type = "triangle")
library(ape)
plot(hcd, type = "fan")
# HoltWinters() funtion for simple exponential smoothing
rain <- ts(scan("https://robjhyndman.com/tsdldata/hurst/precip1.dat", skip = 1), start = c(1813))
plot.ts(rain)
rainF <- HoltWinters(rain, beta = FALSE, gamma = FALSE)
rainF
getTransformations()
corpus <- tm_map(corpus, toSpace,"'")
inspect(corpus[1:3])
inspect(corpus[1:10])
p<- ggplot(subset(wf,freqr>100))
# Frequency Histogram
wf = data.frame(term=names(freqr),occurrences=freqr)
# Find the frequency words
freq <- colSums(as.matrix(dtm))
freq
length(freq)
ord <- order(freq, decreasing = T)
ord
ord <- freq[order(freq, decreasing = T)]
ord
# create sort order(descending)
ord <- order(freq,decreasing = T)
#inspect most frequently occuring terms
freq[head(ord)]
ord <- freq[order(freq, decreasing = T)]
ord
head(ord)
ord <- freq[order(freq, decreasing = T)]
ord
# list the first 6 most frequent words
head(ord)
dtmr <- DocumentTermMatrix(corpus,control = list(wordLengths= c(4,20),bounds=list(global=c(3,27))))
dtmr
data("mtcars")
test_vars <- c("mpg","cyl","disp","hp","wt","gear","carb")
pairs(x= mtcars[,test_vars],
panel = panel.smooth,
main= "asdf")
# prepare hierarchical cluster
hc = hclust(dis(mtcars))
#very simple dengrogram
plot(hc)
# using dendrogram objects
hcd = as.dendrogram(hc)
plot(hcd, type = "triangle")
bestMargin <- ddply(companiesDataNewRound, "company", summarize, bestMargin = max(margin))
fy<- c(2010,2011,2012,2010,2011,2012,2010,2011,2012)
company<- c("Apple","Apple","Apple","Google","Google","Google","Microsoft","Microsoft","Microsoft")
revenue<- c(65225,108249,156508,29321,37905,50175,62484,69943,73723)
profit<- c(14013,25922,41733,8505,9737,10737,18760,23150,16978)
companiesData<- data.frame(fy,company,revenue,profit)
companiesData
margin <- c(profit/revenue*100)
margin
companiesDataNew <- data.frame(companiesData, margin)
companiesDataNew
companiesDataNewRound <- cbind(companiesData,margin = round(margin,1))
companiesDataNewRound
# e4
install.packages("plyr")
install.packages("plyr")
library(plyr)
bestMargin <- ddply(companiesDataNewRound, "company", summarize, bestMargin = max(margin))
bestMargin
bestMarginWhole <- ddply(companiesDataNewRound, "company", transform, bestMargin = max(margin))
bestMarginWhole
# e5
bestMarginWhole2 <- ddply(companiesDataNewRound, "company", function(x) x[x$margin == max(x$margin),])
bestMarginWhole2
# Mining the corpus
freqr <- colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
ordr <- order(freqr,decreasing = T)
#inspect most frequently occuring terms
freqr[head(ordr)]
#inspect least frequently occuring terms
freqr[tail(ordr)]
# using findFreqTerms() function to get a list of terms that occur at least xxx times:
findFreqTerms(dtmr,lowfreq = 80) # however, the result is ordered alphabetically, not by frequency
# using findAssos() to list iterms by frequency
findAssocs(dtmr,"project",0.6) # a correlation limit of 0.6 will return terms that have a search
#term co-occurrence of at least 60% and so on.
findAssocs(dtmr,"enterpris",0.6)
# ============DTM==========(document term matrix)
# creating a DTM in R
dtm <- DocumentTermMatrix(docs)
dtm
# the result shows that this is a 30 x 3970 dimension matrix in which 88% of the rows are 0.
# limit the information displayed, one can inspect a small section of it:
inspect(dtm[1:2,1000:1005]) # this command displays terms 1000 through 1005 in the first two rows of the DTM.Note that your results may differ.
# Mining the corpus
freq <- colSums(as.matrix(dtm)) # convert TDM into a mathematical matrix using the as.matrix().T
#To get the frequency of occurence of each word in the corpus, we simply sum over all rows to give column sums
length(freq)
# create sort order(descending)
ord <- order(freq,decreasing = T)
#inspect most frequently occuring terms
freq[head(ord)]
#inspect least frequently occuring terms
freq[tail(ord)]
# remove some useless words like"one", "can" ,.. by enforcing bounds when creating the DTM:
dtmr <- DocumentTermMatrix(docs,control = list(wordLengths= c(4,20),bounds=list(global=c(3,27))))
# inspecting the new DTM
dtmr
# Mining the corpus
freqr <- colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
ordr <- order(freqr,decreasing = T)
#inspect most frequently occuring terms
freqr[head(ordr)]
#inspect least frequently occuring terms
freqr[tail(ordr)]
# using findFreqTerms() function to get a list of terms that occur at least xxx times:
findFreqTerms(dtmr,lowfreq = 80) # however, the result is ordered alphabetically, not by frequency
# using findAssos() to list iterms by frequency
findAssocs(dtmr,"project",0.6) # a correlation limit of 0.6 will return terms that have a search
#term co-occurrence of at least 60% and so on.
findAssocs(dtmr,"enterpris",0.6)
# Frequency Histogram
wf = data.frame(term=names(freqr),occurrences=freqr)
library(ggplot2)
p<- ggplot(subset(wf,freqr>100))
plot(p)
#e5
amzn_smooth <- HoltWinters(amzn_ts, gamma = F)
amzn_smooth
amzn_forecast <- forecast(amzn_smooth)
plot(amzn_forecast)
